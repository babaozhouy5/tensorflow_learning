{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 使用tensorflow构建京东评论分类系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.爬取淘宝评论数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1爬取代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# get the comments\n",
    "def get_comments(url):\n",
    "    comments = []\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding = 'gbk'\n",
    "    if resp.status_code != 200:\n",
    "        return []\n",
    "    content = resp.text\n",
    "    if content:\n",
    "        comment_infos = json.loads(content[content.find('(')+1:-2])['comments']\n",
    "        for comment_info in comment_infos:\n",
    "            comment_content = comment_info['content']\n",
    "            comments.append(comment_content.encode('utf-8')+'\\n')\n",
    "    return comments\n",
    "\n",
    "# save comments\n",
    "def save_comments(comments, type='good'):\n",
    "    with open(type+'.txt', 'w') as fw:\n",
    "        fw.writelines(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2好评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "good_comments = []\n",
    "good_comment_url_template = 'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv8914&productId=10359162198&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0'\n",
    "\n",
    "# good comments\n",
    "for i in range(1000):\n",
    "    url = good_comment_url_template.format(i)\n",
    "    good_comments += get_comments(url)\n",
    "save_comments(good_comments, type='good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3坏评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bad_comments = []\n",
    "bad_comment_url_templates = [\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv8914&productId=10359162198&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv73&productId=10968941641&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'http://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv4653&productId=10335204102&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv1&productId=1269194114&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2777&productId=1409704820&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv174&productId=10103790891&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv9447&productId=1708318938&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "    'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv111&productId=10849803616&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0'\n",
    "]\n",
    "\n",
    "# bad comments\n",
    "for bad_comment_url_template in bad_comment_url_templates:\n",
    "    for i in range(80):\n",
    "        url = bad_comment_url_template.format(i)\n",
    "        bad_comments += get_comments(url)\n",
    "    save_comments(bad_comments, type='bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.数据整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1创建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.072 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./good.txt has 4450 lines, 55390 words.\n",
      "./bad.txt has 4345 lines, 64872 words.\n",
      "dict size 272\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "good_file = './good.txt'\n",
    "bad_file  = './bad.txt'\n",
    "\n",
    "def create_dictionary(good_file, bad_file, lower=50, upper=1000):\n",
    "    \n",
    "    all_words = []\n",
    "    with open(good_file, 'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            words = jieba.lcut(line)\n",
    "            all_words += words\n",
    "    print '{0} has {1} lines, {2} words.'.format(good_file, idx+1, len(all_words))\n",
    "    \n",
    "    count = len(all_words)\n",
    "    with open(bad_file, 'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            words = jieba.lcut(line)\n",
    "            all_words += words\n",
    "    print '{0} has {1} lines, {2} words.'.format(bad_file, idx+1, len(all_words)-count)\n",
    "    \n",
    "    dict = []\n",
    "    cnt = Counter(all_words)\n",
    "    for word, freq in cnt.iteritems():\n",
    "        if lower <= freq <= upper:\n",
    "            dict.append(word)\n",
    "    print 'dict size {}'.format(len(dict))\n",
    "    return dict\n",
    "\n",
    "dict = create_dictionary(good_file, bad_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive dataset size 4450\n",
      "negtive dataset size 4345\n",
      "dataset length 8795\n",
      "train_set size 7036\n",
      "test_set size 1759\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "\n",
    "def wordbag():\n",
    "    \n",
    "    def line2vec(dict, line, cls):\n",
    "        words = jieba.lcut(line)\n",
    "        features = np.zeros(len(dict))\n",
    "        for word in words:\n",
    "            if word in dict:\n",
    "                features[dict.index(word)] = 1\n",
    "        return [features, cls]\n",
    "    \n",
    "    dataset = []\n",
    "    with open('good.txt', 'r') as fr:\n",
    "        for line in fr:\n",
    "            dataset.append(line2vec(dict, line, [0, 1]))\n",
    "    print 'positive dataset size {}'.format(len(dataset))\n",
    "   \n",
    "    count = len(dataset)\n",
    "    \n",
    "    with open('bad.txt', 'r') as fr:\n",
    "        for line in fr:\n",
    "            dataset.append(line2vec(dict, line, [1, 0]))\n",
    "    print 'negtive dataset size {}'.format(len(dataset)-count)\n",
    "    \n",
    "    print 'dataset length {}'.format(len(dataset))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = wordbag()\n",
    "# print dataset[1]\n",
    "\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "fp = open('comment_data.pkl', 'wb')\n",
    "dataset = np.array(dataset)\n",
    "pickle.dump(dataset, fp)\n",
    "\n",
    "train_size = int(dataset.shape[0]*0.8)\n",
    "train_set  = dataset[:train_size]\n",
    "test_set   = dataset[train_size:]\n",
    "\n",
    "print 'train_set size {}'.format(train_set.shape[0])\n",
    "print 'test_set size {}'.format(test_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.定义前馈(feed forward)神经网络训练评论数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.0引入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1定义神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 定义每一层神经元的个数\n",
    "\"\"\"\n",
    "层数的选择：线性数据使用1层，非线性数据使用2册, 超级非线性使用3+册。层数／神经元过多会导致过拟合\n",
    "\"\"\"\n",
    "n_input_layer  = len(dict) # 输入层\n",
    "n_hidden_layer_1 = 30 # hidden layer 1\n",
    "n_hidden_layer_2 = 20 # hidden layer 2\n",
    "n_output_layer = 2 # 输出层\n",
    "\n",
    "W_xh = tf.Variable(tf.random_normal([n_input_layer, n_hidden_layer_1]))\n",
    "b_h1  = tf.Variable(tf.random_normal([n_hidden_layer_1]))\n",
    "\n",
    "W_hh = tf.Variable(tf.random_normal([n_hidden_layer_1, n_hidden_layer_2]))\n",
    "b_h2  = tf.Variable(tf.random_normal([n_hidden_layer_2]))\n",
    "\n",
    "W_ho = tf.Variable(tf.random_normal([n_hidden_layer_2, n_output_layer]))\n",
    "b_o  = tf.Variable(tf.random_normal([n_output_layer]))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# 定义实际输入和输出数据\n",
    "X = tf.placeholder('float', [None, len(dict)])\n",
    "Y = tf.placeholder('float', [None, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2定义神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_network(x):\n",
    "    \n",
    "    hidden_layer_1_output = tf.matmul(x, W_xh) + b_h1\n",
    "    hidden_layer_1_activate = tf.nn.sigmoid(hidden_layer_1_output)  # 激活函数\n",
    "    \n",
    "    hidden_layer_2_output = tf.matmul(hidden_layer_1_activate, W_hh) + b_h2\n",
    "    hidden_layer_2_output = tf.nn.sigmoid(hidden_layer_2_output)\n",
    "    \n",
    "    output = tf.matmul(hidden_layer_2_output, W_ho) + b_o\n",
    "    output = tf.nn.softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.3训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.807277\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(x, y):\n",
    "    \n",
    "    predict       = neural_network(x)\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(predict), reduction_indices=[1]))\n",
    "    train_step    = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) # 学习率改为0.1，加速梯度下降\n",
    "    init          = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        epochs = 5000\n",
    "        np.random.shuffle(train_set)\n",
    "        \n",
    "        train_x = train_set[:, 0].tolist()\n",
    "        train_y = train_set[:, 1].tolist()\n",
    "\n",
    "        test_x = test_set[:, 0].tolist()\n",
    "        test_y = test_set[:, 1].tolist()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            i  = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end   = i + batch_size\n",
    "                batch_x, batch_y = train_x[start:end], train_y[start:end]\n",
    "                loss = sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
    "                i += end\n",
    "                \n",
    "        correct = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print '准确率：', sess.run(accuracy, feed_dict={x: test_x, y: test_y})\n",
    "\n",
    "train_neural_network(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
