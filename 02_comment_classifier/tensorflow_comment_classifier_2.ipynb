{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 使用tensorflow构建推特评论分类系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading dataset...\n",
      "finish, amount 81363704 bytes!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib2\n",
    "\n",
    "dataset_url = 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
    "\n",
    "print 'downloading dataset...'\n",
    "filename = 'trainingandtestdata.zip'\n",
    "content = urllib2.urlopen(dataset_url).read()\n",
    "open(filename, 'wb').write(content)\n",
    "\n",
    "print 'finish, amount {} bytes!'.format(os.path.getsize(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt has 1600000 lines.\n",
      "test.txt has 498 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "\n",
    "# 选取所需的字段\n",
    "def preprocess(input_file, output_file):\n",
    "    \n",
    "    lines = []\n",
    "    with open(input_file, 'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            line = line.replace('\"', '')\n",
    "            items = line.split(',')\n",
    "            attitude, tweet = items[0], ','.join(items[5:])\n",
    "            lines.append(attitude+':%:%:%:'+tweet)\n",
    "    print '{0} has {1} lines.'.format(output_file, len(lines))\n",
    "    \n",
    "    with open(output_file, 'w') as fw:\n",
    "        fw.writelines(lines)\n",
    "\n",
    "preprocess('./training.1600000.processed.noemoticon.csv', 'train.txt')\n",
    "preprocess('./testdata.manual.2009.06.14.csv', 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. 创建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "data_file = './train.txt'\n",
    "\n",
    "def create_dictionary(data_file, start_rate=0.2, size=112):\n",
    "    \n",
    "    all_words = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    with open(data_file) as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            items = line.split(':%:%:%:')\n",
    "            if len(items) < 2:\n",
    "                continue\n",
    "            words = word_tokenize(items[1].decode('latin-1').lower())\n",
    "            words = [lemmatizer.lemmatize(word) for word in words]\n",
    "            all_words += words\n",
    "    \n",
    "    print '{0} has {1} lines, {2} words.'.format(data_file, idx+1, len(all_words))\n",
    "    \n",
    "    dict = []\n",
    "    cnt = Counter(all_words).most_common()\n",
    "    for idx, (word, freq) in enumerate(cnt[int(len(cnt)*start_rate):]):\n",
    "        if idx >= size:\n",
    "            break\n",
    "        dict.append(word)\n",
    "            \n",
    "    print 'dict size {}'.format(len(dict))\n",
    "    return dict\n",
    "\n",
    "dict = create_dictionary(data_file, start_rate=0.2, size=112)\n",
    "fp = open('tweet_dict.pkl', 'wb')\n",
    "pickle.dump(dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "dict = pickle.load(open('tweet_dict.pkl', 'r'))\n",
    "print len(dict)\n",
    "def next_batch(data, batch_size):\n",
    "    \n",
    "    def line2vec(dict, line):\n",
    "        items = line.split(':%:%:%:')\n",
    "        if len(items) < 2:\n",
    "            return None\n",
    "        attitude, tweet = items\n",
    "        words = word_tokenize(tweet.decode('latin-1').lower())\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        if attitude == '0':\n",
    "            cls = [0, 0, 1]\n",
    "        elif attitude == '2':\n",
    "            cls = [0, 1, 0]\n",
    "        elif attitude == '4':\n",
    "            cls = [1, 0, 0]\n",
    "        else:\n",
    "            cls = [0, 0, 0]\n",
    "        \n",
    "        features = np.zeros(len(dict))\n",
    "        for word in words:\n",
    "            if word in dict:\n",
    "                features[dict.index(word)] = 1\n",
    "        return [features, cls]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    samples = random.sample(data, batch_size)\n",
    "    batch = [line2vec(dict, line) for line in samples]\n",
    "    return np.array(batch)\n",
    "\n",
    "fp = open('./train.txt')\n",
    "train_data = fp.readlines()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.定义前馈(feed forward)神经网络训练推特评论数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.0引入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1定义神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 定义每一层神经元的个数\n",
    "\"\"\"\n",
    "层数的选择：线性数据使用1层，非线性数据使用2册, 超级非线性使用3+册。层数／神经元过多会导致过拟合\n",
    "\"\"\"\n",
    "n_input_layer  = len(dict) # 输入层\n",
    "n_hidden_layer_1 = 30 # hidden layer 1\n",
    "n_hidden_layer_2 = 40 # hidden layer 2\n",
    "n_output_layer = 3 # 输出层\n",
    "\n",
    "W_xh = tf.Variable(tf.random_normal([n_input_layer, n_hidden_layer_1]))\n",
    "b_h1  = tf.Variable(tf.random_normal([n_hidden_layer_1]))\n",
    "\n",
    "W_hh = tf.Variable(tf.random_normal([n_hidden_layer_1, n_hidden_layer_2]))\n",
    "b_h2  = tf.Variable(tf.random_normal([n_hidden_layer_2]))\n",
    "\n",
    "W_ho = tf.Variable(tf.random_normal([n_hidden_layer_2, n_output_layer]))\n",
    "b_o  = tf.Variable(tf.random_normal([n_output_layer]))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# 定义实际输入和输出数据\n",
    "X = tf.placeholder('float', [None, len(dict)])\n",
    "Y = tf.placeholder('float', [None, n_output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2定义网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(x):\n",
    "    \n",
    "    hidden_layer_1_output = tf.matmul(x, W_xh) + b_h1\n",
    "    hidden_layer_1_activate = tf.nn.sigmoid(hidden_layer_1_output)  # 激活函数\n",
    "    \n",
    "    hidden_layer_2_output = tf.matmul(hidden_layer_1_activate, W_hh) + b_h2\n",
    "    hidden_layer_2_output = tf.nn.sigmoid(hidden_layer_2_output)\n",
    "    \n",
    "    output = tf.matmul(hidden_layer_2_output, W_ho) + b_o\n",
    "    output = tf.nn.softmax(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳准确率:  0.365462\n",
      "最佳准确率:  0.36747\n",
      "最佳准确率:  0.375502\n",
      "最佳准确率:  0.383534\n",
      "最佳准确率:  0.395582\n",
      "最佳准确率:  0.405622\n",
      "最佳准确率:  0.411647\n",
      "最佳准确率:  0.423695\n",
      "最佳准确率:  0.427711\n",
      "最佳准确率:  0.429719\n",
      "最佳准确率:  0.431727\n",
      "最佳准确率:  0.437751\n",
      "最佳准确率:  0.441767\n",
      "最佳准确率:  0.449799\n",
      "最佳准确率:  0.455823\n",
      "最佳准确率:  0.463855\n",
      "最佳准确率:  0.465863\n",
      "最佳准确率:  0.467871\n",
      "最佳准确率:  0.477912\n",
      "最佳准确率:  0.481928\n",
      "最佳准确率:  0.483936\n",
      "最佳准确率:  0.485944\n",
      "最佳准确率:  0.491968\n",
      "最佳准确率:  0.493976\n",
      "最佳准确率:  0.497992\n",
      "最佳准确率:  0.5\n",
      "最佳准确率:  0.504016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_neural_network(x, y):\n",
    "    \n",
    "    predict = NeuralNetwork(x)\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(predict), reduction_indices=[1]))\n",
    "    train_step    = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    # AdamOptimizer\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    init  = tf.global_variables_initializer()\n",
    "    \n",
    "    fp = open('./train.txt')\n",
    "    train_data = fp.readlines()\n",
    "    fp.close()\n",
    "    \n",
    "    fp = open('./test.txt')\n",
    "    test_data = fp.readlines()\n",
    "    fp.close()\n",
    "    \n",
    "    test_set = next_batch(test_data, len(test_data))\n",
    "    test_x = test_set[:, 0].tolist()\n",
    "    test_y = test_set[:, 1].tolist()\n",
    "    \n",
    "    pre_accuracy = 0.0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        i = 0\n",
    "        while True:\n",
    "            mini_batch = next_batch(train_data, batch_size)\n",
    "            batch_x = mini_batch[:, 0].tolist()\n",
    "            batch_y = mini_batch[:, 1].tolist()\n",
    "            \n",
    "            sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            if i > 100:\n",
    "                correct = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "                result = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                accuracy = sess.run(result, feed_dict={x: test_x, y: test_y})\n",
    "                # accuracy = accuracy.eval({X:test_x, Y:test_y})\n",
    "                if accuracy > pre_accuracy:  # 保存准确率最高的训练模型\n",
    "                    print '最佳准确率: ', accuracy\n",
    "                    pre_accuracy = accuracy\n",
    "                    saver.save(sess, 'model.ckpt')  # 保存session\n",
    "                i = 0\n",
    "            i += 1\n",
    "\n",
    "train_neural_network(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.4重用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n"
     ]
    }
   ],
   "source": [
    "def predict(tweet):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(tweet.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    input = np.zeros(len(dict))\n",
    "    for word in words:\n",
    "        if word in dict:\n",
    "            input[dict.index(word)] = 1\n",
    "    \n",
    "    input = input.reshape(1, -1)\n",
    "    X = tf.placeholder('float', [None, len(dict)])\n",
    "    output = NeuralNetwork(X)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    init  = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, './model.ckpt')\n",
    "        # prediction = sess.run(output, feed_dict={X: input})\n",
    "        prediction = tf.argmax(output.eval(feed_dict={X: input}), 1)\n",
    "    \n",
    "        if prediction == 2:\n",
    "            attitude = 'good'\n",
    "        elif prediction == 1:\n",
    "            attitude = 'just so so.'\n",
    "        else:\n",
    "            attitude = 'bad'\n",
    "        \n",
    "        print attitude\n",
    "\n",
    "predict('it is very good.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6.定义CNN(卷积神经网络)训练推特评论数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.1定义网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_size  = len(dict)\n",
    "\n",
    "num_classes = 3\n",
    "batch_size  = 100\n",
    "\n",
    "embedding_size = input_size   # the input X's embedding size\n",
    "filter_conf = [(5, 5, 1, 32), (5, 5, 32, 64)] # [(size, in_channel, output_channel)]\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, input_size])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.2定义CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv2d(x, W, bias):\n",
    "    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + bias)\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def ConvNeuralNetwork(x):\n",
    "    \n",
    "    # 1.embedding layer\n",
    "    with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "        Vec_Table = tf.Variable(tf.random_uniform([input_size, embedding_size], -1.0, 1.0))\n",
    "        embeded_vec = tf.expand_dims(tf.nn.embedding_lookup(Vec_Table, x), -1)\n",
    "    \n",
    "    # 2.convolution + maxpool layer\n",
    "    pool_layer = tf.reshape(embeded_vec, [-1, input_size, embedding_size, 1])\n",
    "    for idx, (filter_size, filter_size, in_channel, output_channel) in enumerate(filter_conf):\n",
    "        with tf.name_scope('conv_maxpool_layer_%d' % idx):\n",
    "            filter_shape = [filter_size, filter_size, in_channel, output_channel]\n",
    "            W = weight_variable(filter_shape)\n",
    "            b = bias_variable([output_channel])\n",
    "            conv_layer  = conv2d(pool_layer, W, b)\n",
    "            pool_layer  = max_pool_2x2(conv_layer)\n",
    "\n",
    "    # 3.fully-connected layer\n",
    "    W_fc1 = weight_variable([28*28*64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    pool_flat = tf.reshape(pool_layer, [-1, 28*28*64])\n",
    "    fc_output = tf.nn.relu(tf.matmul(pool_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # 4.dropout layer\n",
    "    drop_output = tf.nn.dropout(fc_output, keep_dropout_prob)\n",
    "    \n",
    "    # 5.readout layer\n",
    "    W_fc2 = weight_variable([1024, num_classes])\n",
    "    b_fc2 = bias_variable([num_classes])\n",
    "    output = tf.nn.softmax(tf.matmul(drop_output, W_fc2) + b_fc2)\n",
    "  \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.3训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前准确率： 0.365462\n",
      "最佳准确率:  0.365462\n",
      "当前准确率： 0.365462\n",
      "当前准确率： 0.365462\n",
      "当前准确率： 0.365462\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_neural_network(x, y):\n",
    "    \n",
    "    predict = ConvNeuralNetwork(x)\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(predict), reduction_indices=[1]))\n",
    "    # cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) # do not need tf.nn.softmax in the Readout Layer\n",
    "\n",
    "    train_step    = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    # AdamOptimizer\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    init  = tf.global_variables_initializer()\n",
    "    \n",
    "    fp = open('./train.txt')\n",
    "    train_data = fp.readlines()\n",
    "    fp.close()\n",
    "    \n",
    "    fp = open('./test.txt')\n",
    "    test_data = fp.readlines()\n",
    "    fp.close()\n",
    "    \n",
    "    test_set = next_batch(test_data, len(test_data))\n",
    "    test_x = test_set[:, 0].tolist()\n",
    "    test_y = test_set[:, 1].tolist()\n",
    "    \n",
    "    pre_accuracy = 0.0\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while True:\n",
    "        mini_batch = next_batch(train_data, batch_size)\n",
    "        batch_x = mini_batch[:, 0].tolist()\n",
    "        batch_y = mini_batch[:, 1].tolist()\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y: batch_y, keep_dropout_prob: 0.5})\n",
    "\n",
    "        if i > 100:\n",
    "            correct = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "            result = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            accuracy = sess.run(result, feed_dict={x: test_x, y: test_y, keep_dropout_prob: 1.0})\n",
    "            print '当前准确率：', accuracy\n",
    "            # accuracy = accuracy.eval({X:test_x, Y:test_y})\n",
    "            if accuracy > pre_accuracy:  # 保存准确率最高的训练模型\n",
    "                print '最佳准确率: ', accuracy\n",
    "                pre_accuracy = accuracy\n",
    "                saver.save(sess, 'model.ckpt')  # 保存session\n",
    "            i = 0\n",
    "        i += 1\n",
    "\n",
    "train_neural_network(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
