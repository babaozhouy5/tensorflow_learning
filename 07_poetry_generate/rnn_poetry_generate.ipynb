{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 使用RNN训练一个古诗词生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐诗总数： 34813\n",
      "不重复字总数： 6122\n",
      "[3, 279, 573, 114, 422, 973, 0, 487, 104, 468, 872, 1209, 1, 10, 14, 226, 212, 3451, 0, 31, 49, 98, 6, 12, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from collections import Counter\n",
    "\n",
    "# 1.过滤\n",
    "poetrys = []\n",
    "with codecs.open('./poetry.txt', encoding='utf-8') as fr:\n",
    "    for line in fr:\n",
    "        try:\n",
    "            title, content = line.strip().split(':')\n",
    "            content = content.replace(' ', '')\n",
    "            if u'（' in content or u'(' in content or u'《' in content or u'_' in content or u'[' in content:\n",
    "                continue\n",
    "            if len(content) < 5 or len(content) > 80:\n",
    "                continue\n",
    "            content = '[' + content + ']'\n",
    "            poetrys.append(content)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "print u'唐诗总数：', len(poetrys)\n",
    "poetrys = sorted(poetrys, key=lambda x: len(x))\n",
    "\n",
    "# 2.构建word和id的双向映射表\n",
    "cnt = Counter(''.join(poetrys)).most_common()\n",
    "words_sorted_by_freq = zip(*cnt)[0] + (' ', )\n",
    "print '不重复字总数：', len(words_sorted_by_freq)\n",
    "id2word = dict(enumerate(words_sorted_by_freq))\n",
    "word2id = dict(zip(id2word.values(), id2word.keys()))\n",
    "\n",
    "# 3.将诗歌转为id向量形式\n",
    "poetrys_vec = [[word2id[word] for word in poetry] for poetry in poetrys]\n",
    "word_nums = len(words_sorted_by_freq)\n",
    "print poetrys_vec[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.定义batch函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def next_batch(data, batch_size, num_steps):\n",
    "    chunk_nums = len(data) / batch_size\n",
    "    batch_len = chunk_nums if num_steps == -1 else min(chunk_nums, num_steps)\n",
    "    for i in range(batch_len):\n",
    "        batches = data[i*batch_size:(i+1)*batch_size]\n",
    "        feature_nums = max(map(len, batches))\n",
    "        x = np.full((batch_size, feature_nums), word2id[' '], np.int32)\n",
    "        for row, poetry in enumerate(batches):\n",
    "            x[row, :len(poetry)] = poetry\n",
    "        y = np.copy(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "思考：对于RNN来讲，**rnn_size(hidden state size)**与输入序列的**元素**的嵌入向量维度相同（此处与字的嵌入维度相关<比如使用BOW模型>），但是由于序列的长度长短不一，这不利于RNN的训练，因此**一般**都会对输入序列做**padding**操作（这样带来选择：1.手动做padding处理 2.使用tensorflow做处理(**dynamic_rnn甚至可以动态构建，而不需要padding**)），使序列长度均为**time_step**步；\n",
    "\n",
    "具体信息，请参见：\n",
    " - http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    " - http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.定义RNN网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1.引入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "# reset_graph()\n",
    "# tf.contrib.seq2seq.sequence_loss?\n",
    "# tf.variable_scope?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2.定义RNN网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def MultiLayerRNN(cell_type='lstm', hstate_size=128, layer_nums=2, learning_rate=1e-4, batch_size=64):\n",
    "    \n",
    "    if cell_type == 'lstm':\n",
    "        cell_func = tf.contrib.rnn.BasicLSTMCell\n",
    "    elif cell_type == 'gru':\n",
    "        cell_func = tf.contrib.rnn.GRUCell\n",
    "    elif cell_type == 'rnn':\n",
    "        cell_func = tf.contrib.rnn.BasicRNNCell\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, [None, None]) # 第一个是batch_size；第二个是序列长度，即time_step(由于这个是不定长的，所以是None)\n",
    "    y = tf.placeholder(tf.int32, [None, None])\n",
    "\n",
    "    # 1.embedding layer\n",
    "    with tf.variable_scope('embedding'):\n",
    "        embeddings = tf.get_variable('embedding_matrix', [word_nums, hstate_size])\n",
    "        # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    # 2.定义rnn layer\n",
    "    cell = cell_func(hstate_size, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*layer_nums, state_is_tuple=True)\n",
    "    initial_cstate = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_cstate = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=initial_cstate)\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [hstate_size, word_nums])\n",
    "        b = tf.get_variable('b', [word_nums], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    # reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, hstate_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    # 3.定义output layer\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "    probs  = tf.nn.softmax(logits)\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))\n",
    "    #train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    # 手动控制梯度的传播\n",
    "    tvars = tf.trainable_variables()\n",
    "    # grads_and_tvars = tf.compute_gradients(total_loss, tvars)\n",
    "    # grads = zip(*grads_and_tvars)[0]\n",
    "    grads = tf.gradients(total_loss, tvars)\n",
    "    clipped_grads, _ = tf.clip_by_global_norm(grads, 5)\n",
    "    opti  = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = opti.apply_gradients(zip(clipped_grads, tvars))\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        initial_cstate = initial_cstate,\n",
    "        final_cstate = final_cstate,\n",
    "        probs = probs,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        saver = saver\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.3.定义训练RNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps = -1, batch_size = 64, verbose = True, save=False):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in next_batch(poetrys_vec, batch_size, num_steps):\n",
    "                steps += 1\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['initial_cstate']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_cstate'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch {}/{}:{}\".format(epoch+1, num_epochs, training_loss/steps))\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "            if save and (epoch+1) % 5 == 0:\n",
    "                g['saver'].save(sess, 'poetry_gen_model', global_step=epoch)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.5.训练RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 1/100:6.86393547409\n",
      "Average training loss for Epoch 2/100:6.42438518584\n",
      "Average training loss for Epoch 3/100:6.41937117111\n",
      "Average training loss for Epoch 4/100:6.41712725272\n",
      "Average training loss for Epoch 5/100:6.4161247847\n",
      "Average training loss for Epoch 6/100:6.4156662667\n",
      "Average training loss for Epoch 7/100:6.41529980285\n",
      "Average training loss for Epoch 8/100:6.4113553769\n",
      "Average training loss for Epoch 9/100:6.36540350642\n",
      "Average training loss for Epoch 10/100:6.30113961025\n",
      "Average training loss for Epoch 11/100:6.14469979306\n",
      "Average training loss for Epoch 12/100:6.03695265189\n",
      "Average training loss for Epoch 13/100:5.98728574661\n",
      "Average training loss for Epoch 14/100:5.94861563705\n",
      "Average training loss for Epoch 15/100:5.89806809873\n",
      "Average training loss for Epoch 16/100:5.82521183934\n",
      "Average training loss for Epoch 17/100:5.78797029702\n",
      "Average training loss for Epoch 18/100:5.76041570597\n",
      "Average training loss for Epoch 19/100:5.73833363798\n",
      "Average training loss for Epoch 20/100:5.7199812155\n",
      "Average training loss for Epoch 21/100:5.69966038112\n",
      "Average training loss for Epoch 22/100:5.6804684293\n",
      "Average training loss for Epoch 23/100:5.65833728748\n",
      "Average training loss for Epoch 24/100:5.63779233174\n",
      "Average training loss for Epoch 25/100:5.62391608942\n",
      "Average training loss for Epoch 26/100:5.61391154003\n",
      "Average training loss for Epoch 27/100:5.59749277389\n",
      "Average training loss for Epoch 28/100:5.58404487009\n",
      "Average training loss for Epoch 29/100:5.5702980254\n",
      "Average training loss for Epoch 30/100:5.55729715337\n",
      "Average training loss for Epoch 31/100:5.54354493175\n",
      "Average training loss for Epoch 32/100:5.53071364777\n",
      "Average training loss for Epoch 33/100:5.51864984681\n",
      "Average training loss for Epoch 34/100:5.50638111828\n",
      "Average training loss for Epoch 35/100:5.4955081404\n",
      "Average training loss for Epoch 36/100:5.48401805797\n",
      "Average training loss for Epoch 37/100:5.47299304017\n",
      "Average training loss for Epoch 38/100:5.4635661004\n",
      "Average training loss for Epoch 39/100:5.45257845153\n",
      "Average training loss for Epoch 40/100:5.44243072893\n",
      "Average training loss for Epoch 41/100:5.43218906764\n",
      "Average training loss for Epoch 42/100:5.42184637748\n",
      "Average training loss for Epoch 43/100:5.41103868941\n",
      "Average training loss for Epoch 44/100:5.39995131308\n",
      "Average training loss for Epoch 45/100:5.38868016477\n",
      "Average training loss for Epoch 46/100:5.37742632748\n",
      "Average training loss for Epoch 47/100:5.3665183675\n",
      "Average training loss for Epoch 48/100:5.35539175704\n",
      "Average training loss for Epoch 49/100:5.34425819864\n",
      "Average training loss for Epoch 50/100:5.3360082045\n",
      "Average training loss for Epoch 51/100:5.32060695178\n",
      "Average training loss for Epoch 52/100:5.30913445611\n",
      "Average training loss for Epoch 53/100:5.29639676131\n",
      "Average training loss for Epoch 54/100:5.28351816244\n",
      "Average training loss for Epoch 55/100:5.27039578123\n",
      "Average training loss for Epoch 56/100:5.25615348342\n",
      "Average training loss for Epoch 57/100:5.24217711935\n",
      "Average training loss for Epoch 58/100:5.22774281159\n",
      "Average training loss for Epoch 59/100:5.21334279484\n",
      "Average training loss for Epoch 60/100:5.19946056528\n",
      "Average training loss for Epoch 61/100:5.18519439574\n",
      "Average training loss for Epoch 62/100:5.17176783722\n",
      "Average training loss for Epoch 63/100:5.15799595901\n",
      "Average training loss for Epoch 64/100:5.14511247322\n",
      "Average training loss for Epoch 65/100:5.13212112564\n",
      "Average training loss for Epoch 66/100:5.11988767164\n",
      "Average training loss for Epoch 67/100:5.10767758705\n",
      "Average training loss for Epoch 68/100:5.09597025392\n",
      "Average training loss for Epoch 69/100:5.08398264943\n",
      "Average training loss for Epoch 70/100:5.0725712179\n",
      "Average training loss for Epoch 71/100:5.06115751021\n",
      "Average training loss for Epoch 72/100:5.04984489111\n",
      "Average training loss for Epoch 73/100:5.03851776668\n",
      "Average training loss for Epoch 74/100:5.02821345619\n",
      "Average training loss for Epoch 75/100:5.0178575103\n",
      "Average training loss for Epoch 76/100:5.00813215477\n",
      "Average training loss for Epoch 77/100:4.99835035155\n",
      "Average training loss for Epoch 78/100:4.98921286335\n",
      "Average training loss for Epoch 79/100:4.98007507131\n",
      "Average training loss for Epoch 80/100:4.97143085578\n",
      "Average training loss for Epoch 81/100:4.96289343597\n",
      "Average training loss for Epoch 82/100:4.95476230688\n",
      "Average training loss for Epoch 83/100:4.94679987979\n",
      "Average training loss for Epoch 84/100:4.93919238113\n",
      "Average training loss for Epoch 85/100:4.93166468991\n",
      "Average training loss for Epoch 86/100:4.92445797156\n",
      "Average training loss for Epoch 87/100:4.91708501004\n",
      "Average training loss for Epoch 88/100:4.90994959773\n",
      "Average training loss for Epoch 89/100:4.90255033289\n",
      "Average training loss for Epoch 90/100:4.89562560269\n",
      "Average training loss for Epoch 91/100:4.88839792766\n",
      "Average training loss for Epoch 92/100:4.88170450256\n",
      "Average training loss for Epoch 93/100:4.87475755816\n",
      "Average training loss for Epoch 94/100:4.86819486987\n",
      "Average training loss for Epoch 95/100:4.86145734172\n",
      "Average training loss for Epoch 96/100:4.85502584248\n",
      "Average training loss for Epoch 97/100:4.84846104574\n",
      "Average training loss for Epoch 98/100:4.84221994679\n",
      "Average training loss for Epoch 99/100:4.83585296547\n",
      "Average training loss for Epoch 100/100:4.82975154599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.8639354740936671,\n",
       " 6.424385185838843,\n",
       " 6.4193711711117798,\n",
       " 6.4171272527227528,\n",
       " 6.4161247847049498,\n",
       " 6.4156662666995219,\n",
       " 6.4152998028539159,\n",
       " 6.4113553769022058,\n",
       " 6.3654035064177403,\n",
       " 6.3011396102483763,\n",
       " 6.1446997930551319,\n",
       " 6.0369526518862111,\n",
       " 5.9872857466147966,\n",
       " 5.948615637053881,\n",
       " 5.8980680987321215,\n",
       " 5.825211839342205,\n",
       " 5.7879702970248337,\n",
       " 5.7604157059671248,\n",
       " 5.7383336379822225,\n",
       " 5.7199812154963094,\n",
       " 5.6996603811204327,\n",
       " 5.6804684292984708,\n",
       " 5.6583372874813183,\n",
       " 5.6377923317377077,\n",
       " 5.6239160894249904,\n",
       " 5.6139115400296768,\n",
       " 5.5974927738885194,\n",
       " 5.5840448700920655,\n",
       " 5.5702980254016969,\n",
       " 5.5572971533675224,\n",
       " 5.5435449317454415,\n",
       " 5.5307136477686427,\n",
       " 5.5186498468093452,\n",
       " 5.5063811182756233,\n",
       " 5.4955081404023849,\n",
       " 5.4840180579689548,\n",
       " 5.4729930401726543,\n",
       " 5.4635661003997971,\n",
       " 5.4525784515324656,\n",
       " 5.4424307289264036,\n",
       " 5.4321890676438702,\n",
       " 5.4218463774763634,\n",
       " 5.4110386894113667,\n",
       " 5.3999513130820258,\n",
       " 5.388680164765697,\n",
       " 5.3774263274823326,\n",
       " 5.3665183675003751,\n",
       " 5.3553917570447833,\n",
       " 5.3442581986415014,\n",
       " 5.3360082045005388,\n",
       " 5.3206069517750327,\n",
       " 5.3091344561146547,\n",
       " 5.2963967613093761,\n",
       " 5.2835181624410783,\n",
       " 5.2703957812342637,\n",
       " 5.2561534834171528,\n",
       " 5.2421771193516626,\n",
       " 5.2277428115929032,\n",
       " 5.2133427948363362,\n",
       " 5.1994605652754702,\n",
       " 5.1851943957432418,\n",
       " 5.1717678372153042,\n",
       " 5.1579959590132063,\n",
       " 5.1451124732226514,\n",
       " 5.1321211256374974,\n",
       " 5.1198876716374908,\n",
       " 5.107677587049003,\n",
       " 5.0959702539180523,\n",
       " 5.0839826494290685,\n",
       " 5.0725712179039943,\n",
       " 5.061157510205966,\n",
       " 5.0498448911073242,\n",
       " 5.0385177666750183,\n",
       " 5.0282134561907519,\n",
       " 5.0178575103015088,\n",
       " 5.008132154770319,\n",
       " 4.9983503515549126,\n",
       " 4.9892128633530763,\n",
       " 4.9800750713102504,\n",
       " 4.9714308557808948,\n",
       " 4.96289343596822,\n",
       " 4.9547623068807756,\n",
       " 4.9467998797941819,\n",
       " 4.9391923811334708,\n",
       " 4.9316646899088088,\n",
       " 4.9244579715623384,\n",
       " 4.9170850100438237,\n",
       " 4.9099495977327967,\n",
       " 4.9025503328931048,\n",
       " 4.895625602694067,\n",
       " 4.8883979276618463,\n",
       " 4.8817045025623527,\n",
       " 4.8747575581622611,\n",
       " 4.8681948698686632,\n",
       " 4.8614573417228009,\n",
       " 4.8550258424839701,\n",
       " 4.8484610457446697,\n",
       " 4.8422199467927713,\n",
       " 4.8358529654655671,\n",
       " 4.8297515459921261]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "g = MultiLayerRNN()\n",
    "train_network(g, 100, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.6.诗词生成（RNN模型重用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生东山市雀客，四邻来四邻。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_poetry():\n",
    "    \n",
    "    def prob2word(weights):\n",
    "        cum_weights = np.cumsum(weights)\n",
    "        sum_weights = np.sum(weights)\n",
    "        idx = np.searchsorted(cum_weights, np.random.rand(1)*sum_weights)[0]\n",
    "        # wordid = np.random.choice(words_sorted_by_freq, 1, p=probs)[0] # sum of probs is not be 1\n",
    "        return id2word[idx]\n",
    "    \n",
    "    reset_graph()\n",
    "    g = MultiLayerRNN(batch_size=1)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g['saver'].restore(sess, './poetry_gen_model-55')\n",
    "\n",
    "        X = np.zeros((1, 1), dtype=np.int32)\n",
    "        X[0, 0] = word2id['[']\n",
    "        feed_dict = {g['x']: X}\n",
    "        probs_, state_ = sess.run([g['probs'], g['final_cstate']], feed_dict=feed_dict)\n",
    "        word = prob2word(probs_)\n",
    "        poetry = ''\n",
    "        while word != ']':\n",
    "            poetry += word\n",
    "            X[0, 0] = word2id[word]\n",
    "            feed_dict = {g['x']: X, g['initial_cstate']: state_}\n",
    "            probs_, state_ = sess.run([g['probs'], g['final_cstate']], feed_dict=feed_dict)\n",
    "            word = prob2word(probs_)   \n",
    "        return poetry\n",
    "    \n",
    "print gen_poetry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.7.藏头诗生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一战眼，二壁。\n",
      "三守，四。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_poetry_with_head(heads):\n",
    "    \n",
    "    def prob2word(weights):\n",
    "        cum_weights = np.cumsum(weights)\n",
    "        sum_weights = np.sum(weights)\n",
    "        idx = np.searchsorted(cum_weights, np.random.rand(1)*sum_weights)[0]\n",
    "        return id2word[idx]\n",
    "    \n",
    "    reset_graph()   \n",
    "    g = MultiLayerRNN(batch_size=1)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g['saver'].restore(sess, './poetry_gen_model-59')\n",
    "\n",
    "        i = 0\n",
    "        state_ = None\n",
    "        poetry = ''\n",
    "        X = np.zeros((1, 1), dtype=np.int32)\n",
    "        for word in heads:\n",
    "            while word != u'，' and word != u'。':\n",
    "                poetry += word\n",
    "                X[0, 0] = word2id[word]\n",
    "                feed_dict = {g['x']: X}\n",
    "                if state_ is not None:\n",
    "                    feed_dict[g['initial_cstate']] = state_\n",
    "                probs_, state_ = sess.run([g['probs'], g['final_cstate']], feed_dict=feed_dict)\n",
    "                word = prob2word(probs_)\n",
    "            if i % 2 == 0:\n",
    "                poetry += u'，'\n",
    "            else:\n",
    "                poetry += u'。\\n'\n",
    "            i += 1\n",
    "        return poetry\n",
    "    \n",
    "print gen_poetry_with_head(u'一二三四')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.8.网络服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import web\n",
    "\n",
    "render = web.template.render('template/')\n",
    "urls = (\n",
    "    '/', 'index'\n",
    ")\n",
    "\n",
    "class index:\n",
    "    def GET(self):\n",
    "        heads = web.input.heads\n",
    "        poem = gen_poetry_with_head(head)\n",
    "        return render.index(poem)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app = web.application(urls, globals())\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
